---
title: "Predicting a Numerical Outcome With Linear Models"
author: "Paramjot Singh"
date: 2018-03-30T20:53:08-05:00
draft: FALSE
tags: ["R"]
categories: ["Fundamentals", "Machine Learning"]
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  comment = "#>",
  class.source = 'white',
  fig.align = 'center'
)
```
In an earlier post, we learned a little bit about inference and some interesting tools that we can employ to make informed decisions based on our data instead of just guessing. Next you show your analysis to your friend who suggested to look for small engine cars to get better fuel economy. He is extremely impressed by the power of statistics and asks you another question. 

> Can we use the data we have to *predict* the fuel economy of any car from that population that we want?

This and related type of questions fall under the subject of *prediction* which is the subject of this blog. After you answer yes to your friend, he gets really excited and starts to ask a number of questions to which you really don't know the answer to. So you start to explore the subject to get a good understanding yourself first. You immediately realize that the subject is vast, and once again to stay focused, you narrow your work to only *numerical* outcomes and *linear* models.

> It's tough to make predictions, especially about the future. - Yogi Berra

### A Constant Output Machine

Imagine you are tasked to come up with a solution to predict the highway fuel economy of a car. Although we have a full data set (`mpg` from `ggplot2` package), let's pretend for a second that the only information I have is the highway fuel economy of a sample:

```{r}
library(tidyverse)
mpg %>% select(hwy)  
```
The simplest machine that we can conceive is a *constant output machine* having the following form:

$$
 Y \approx C_0
$$

Here, $Y$ is the actual output and $C_0$ is the constant answer. Note that we are approximating our output with this constant answer as the actual output cannot be described with a *deterministic* relationship.

So we have 234 *observations* in all. Without any other information available to us, what constant output can we come up with? We know the best way to describe the entire data is with *mean* which we can estimate by averaging the highway fuel economy data.

```{r}
mpg %>% select(hwy) %>%
    summarize(mean = mean(hwy))
```
In addition, we also know that what we have just computed is the sample mean which is only an estimate of poulation mean. Let's identify this estimate of the constant and the *predicted* value of $Y$ by a hat symbol on top of each.

$$
\hat y = \hat C_0
$$
Let's also calculate the confidence intervals for the estimate. 

```{r}
mpg %>% select(hwy) %>%
    pull() %>%
    t.test() 
```
So we are 95% confidence that the population mean is between 22.7 and 24.2 mpg. It is instructive to draw a picture of what we have built. Let's say you have actually built a machine that takes an information (model, make, year, etc.) as *inputs* ($x$) and gives predicted fuel economy as *output* ($\hat y$). Our current version of the machine gives only constant output.

```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("../images/mpg_1.png")

```
Alright, so we have our first answer: 23.4 mpg. While the answer is a good first guess, it's probably not very accurate. So the next logical is...

### How Accurate is our Prediction?

One way to assess the accuracy is to look at the difference between the actual and predicted value of the output. This difference is referred to as *residuals*.

```{r, out.width = "80%"}
library(themeSimple)
theme_set(theme_simple())
mpg %>% mutate(err_hwy = hwy - mean(hwy)) %>%
    ggplot(aes(hwy, err_hwy)) +
    scatter() +
    geom_hline(yintercept = 0, 
               color = "firebrick", 
               linetype = "dashed")
```
We can also add the square of all residuals to get a single number to represent the overall error. Let's call this number conveniently as *residual sum of squares* (RSS).
```{r}
mpg %>% mutate(err_hwy = hwy - mean(hwy)) %>%
    summarize(rss = sum(err_hwy^2))
```

We are as far off as 20 mpg! This is not unexpected since our prediction machine is really simple with one constant output. Since we have some additional variables in the data, we start to look into whether any of these are related to highway fuel economy. If we find that some variables are related to the output, we may be able to make improvements to our prediction machine. Let's look at our variables:

```{r, out.width = "80%"}
mpg %>% select(hwy, displ, year, cyl) %>% 
    pairs()
```
The top row of pair plots show highway fuel economy on the y-axis and displacement, year and cylinder on x-axis respectively. We learn a few things looking at these plots:

* As displacement increases, fuel economy decreases.
* There is no clear difference in fuel economy between 1999 and 2008 model year vehicles.
* More number of cylinders are generally associated with lower fuel economy.

Because of these relationships, there is a chance that we can improve our prediction machine if we find some way of including the effect of these inputs. A simpler (and thus common) approach is to use a technique called *linear regression*. If the linear regression uses only *one input* then we call this method as *simple linear regression*. It is easy to build a simple linear regression machine but let's first try to look at its form.

### A Simple Linear Regression Machine

Since there is a relationship between fuel economy and displacement, let's see if we can use this information to build a simple linear regression machine which takes the form:

$$
Y \approx C_0 + C_1X
$$

In our case $X$ is displacement. 
```{r}
mpg %>% ggplot(aes(displ, hwy)) +
    scatter()
```
The problem of building a simple linear regression machine then boils down to *estimating* two constants (instead of one in our constant output machine). Once we have estimated these two constants, we can make predictions using:

$$
\hat y = \hat C_0 + \hat C_1 X
$$

Let's estimate these coefficients using `lm` function:

```{r}
mdl_slr <- lm(hwy ~ displ, data = mpg)
mdl_slr
```
**What did *lm* function do to estimate the constants?**

`lm` function is simply trying to *minimize the RSS* to estimate these constants.

**How good are these estimates?**

Well, behind the scenes, *lm* is doing the hypothesis testing assuming t-distribution for these constants. Thus, lm also provides the values of *standard error*, *t-statistic* as well as *p-value* associated with hypothesis test for each constant. This let's us evaluate whether the constants are significant or not. Let's look at this information for our model using `summary` function:

```{r}
summary(mdl_slr)
```
Looking at the coefficients table, it is clear that both constants have extremely small p-values and thus are significant for this data.

**Do These Constants Even Mean Something?**

One advantage of using linear machines is that the constants can be interpreted more meaningfully. The value of $C_0$ is the value of $Y$ when $X$ is 0. So whether this makes sense or not depends on whether the value of $X$ being 0 is meaningful. In our case, since $X$ is the displacement which cannot be 0, $C_0$ alone does not carry any meaning. The value of $C_1$ tells the change that we can expect in $Y$ *per unit change in $X$*. So in our case, $C_0$ of -3.5 means that with a 1 litre increase in engine displacement, we can expect the highway fuel economy to go down by roughly 3.5 mpg. Also, since the *SE* of $C_1$ is 0.19, the 95% confidence interval of our estimate is $3.5 \pm 2 \times 0.19 = [3.12, 3.88]$.

**How Can We Make Predictions Using the Simple Linear Machine?**

We can use `predict` function to get the predictions from our linear machine.
```{r}
pred_hwy <- predict(mdl_slr) 
mpg %>% mutate(res_hwy = hwy - pred_hwy) %>%
    ggplot(aes(hwy, res_hwy)) +
    scatter() +
    geom_hline(yintercept = 0, 
               color = "firebrick", 
               linetype = "dashed")
RSS <- sum((mpg$hwy - pred_hwy)^2)
RSS
```
So our RSS for this data has gone down from 8262 for constant-output machine to 3414 for this simple-linear machine. This is huge improvement! Although RSS represents a single number for the overall accuracy, it is not very interpretable. So now we define two important accuracy metrics for our machine.

**Using $RSE$ and $R^2$ Statistic as Metrics for Prediction Accuracy**

* **Residual Standard Error**: RSE is simply the square root of RSS divided by $n-2$. This tells us how much, on average, we will be off in our predictions using this machine *even when the two constant were exactly known*. RSE is printed at the bottom in the output of summary call to our model. So in our case, we will be off by roughly 3.84 mpg in our prediction. Since the mean mpg (from the constant output model) was 23.4 mpg, the percentage error in our prediction is $(3.84 \times 100) \div 23.4 = 16\%$.

* **$R^2$ Statistic**: While RSE can be interpreted in relation to the units of $Y$, another useful metric that is independent of the units of $Y$ is $R^2$. If we define *total sum of squares*, TSS, as the sum of squares of difference between the actual output and the mean of outputs (which measures the total variance in the output), $R^2$ then is defined as the difference of TSS and RSS divided by TSS. So in this sense, $R^2$ is the proportion of the total variability in the output that can be explained by $X$. From the summary again, in our case, roughly 58% of the variability in mpg can be explained by displacement.
