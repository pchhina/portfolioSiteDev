---
title: "Statistical Inference Using R"
author: "Paramjot Singh"
date: 2018-03-22T03:40:34-05:00
draft: FALSE
tags: ["R"]
categories: ["Fundamentals"]
output:
  blogdown::html_page:
    toc: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  comment = "#>",
  class.source = 'white',
  fig.align = 'center'
)
```
Imagine you were in the market for a new car. You care about running cost of the car and thus one of the buying criteria you have in mind is better fuel economy. One of your friend suggests to buy a 4-cylinder car instead of 6 or 8 cylinders since larger engine size usually translates to worse fuel economy. You are a data scientist so you just don't take his word but want to look at some concrete data before you narrow down your search to 4-cylinder vehicles. You find this `mpg` dataset in one of the R packages, `ggplot2` and start to look at it to answer your question:

> Do 4-cylinder cars have better fuel economy than 6-cylinder cars?

This is a type of question that statistical inference will help us answer. In this post, we will learn the fundamental concepts related to inference. As usual, we will keep the focus tight on the minimum set of tools needed to start applying the concepts to data at hand. We will add more tools as we start to play with different types of data.

### Everything Varies
Variation is inherent to all phenomenon and processes. Otherwise our job is done. Our objective is to understand the variation in data so we can make an informed decision. So we want to 'quantify' the variation in our data. Just looking at the raw data is not going to cut it. Let's try:

```{r}
library(tidyverse)
hwy_4 <- mpg %>% 
    filter(cyl == 4) %>% 
    select(hwy)
hwy_6 <- mpg %>% 
    filter(cyl == 6) %>% 
    select(hwy)
hwy_4
hwy_6
```

It is not a bad idea to look at the raw data but it is difficult to conclude anything. Surprisingly, the fuel economy numbers are rounded to 2 significant figures (and also of type *integer*). Generally, mpg numbers are provided with 3 significant figures. It appears that 4-cylinder vehicles have slightly better fuel economy but there is some overlap too. Moreover, our brain will start to complain if we stare at 80 data points for each set for too long. And sometimes we may have to work with many more observations. One tool to ease out that pain is plots. Let's look at the histogram of each set:

```{r, out.width = "80%"}
library(themeSimple)
ggplot(hwy_4, aes(hwy)) +
    histogram(binwidth = 3) + 
    xlim(15, 45) +
    theme_simple()

ggplot(hwy_6, aes(hwy)) +
    histogram(binwidth = 3) +
    xlim(15, 45) +
    theme_simple()
```
I kept the x-axis scale same in both plots so we can compare the two easily. It is apparent that the mean of 4-cylinder vehicles is centered aroung 30 mpg (miles per gallon) while it is around 25 mpg for 6-cylinder vehicles. So should you conclude that 4-cylinder cars are better? Well, let's wait until we get some quantifiable differences.

### Mean - One Measure of Central Tendency
To quantify the difference, one measure we can consider is *mean*, which is a measure of central tendency of the data:

```{r}
mpg %>% 
    group_by(cyl) %>%
    filter(cyl == 4 | cyl == 6) %>%
    summarize(mean_hwy = mean(hwy))
```
So the mean fuel economy of 4-cylinder cars is greater than 6-cylinder cars by 6 mpg! That's quite a bit. And now we have quantified the difference. Can we conclude now that 4-cylinder cars are better than 6-cylinders? Well not so fast!

### Sample vs Population
One thing is clear that you are not going to pick up one of the cars used to generate this data. For one, the latest model (2008) in the data set is 10 years old. The data we have with us is called *sample* and the mean we have calculated is called *sample mean*. This sample comes from *population* that in this case consists of all the 4 and 6 cylinder cars sold in US from year 1999 to 2008. What we really want to know is *population mean*. But there is no way to exactly know the population mean (unless you are ready to measure mpg for all the cars, which is a costly proposition!). The only option we have is to *estimate* population mean from sample mean. Any sample quantity used to estimate the corresponding population parameter is called a *point estimate*.

### Estimating a Population Parameter from a Sample Quantity
**How do we estimate population mean from sample mean?**

The answer is quite simple here.

> Sample mean is an *unbiased estimator* of population mean.

Sample mean is an *unbiased* estimator of population mean so we don't have to worry too much about the estimate here. What we need to answer is *how confident* are we in our estimate? It is likely that if we collect another sample, the mean of that sample is not going to be exactly same as the first. This is because since the individual observations are *random numbers*, anything that we calculate from those will also be a random number. To understand the variability in mean, we need to know a bit about *variance*.

### Variance - A Measure of Spread
While mean gives you central tendency of a population, *variance* represents the spread or distribution of observations around the mean. We typically think of variance in terms of *standard deviation* which is just the square root of variance. This is because the units of standard deviation matches with the data we are analyzing which makes it easy to compare/interpret. So in short, a data with standard deviation of 5 will be more spread out compared to a data with standard deviation of 1. Let's look at the histogram of these two sets:

```{r, out.width = "80%"}
low_var <- rnorm(1000)
high_var <- rnorm(1000, mean = 0, sd = 5)
data <- tibble(id = rep(c("low", "high"), each = 1000), 
               obs = c(low_var, high_var))
ggplot(data, aes(x = obs)) +
    histogram(data = filter(data, id == "high"), 
              fill = "darkorange",
              alpha = 0.8) +
    histogram(data = filter(data, id == "low"), 
              alpha = 0.8) +
    theme_simple()
```

### What is the Variance in Estimate of Mean
Now that we know what variance is, we need to find what is the variance in our estimate of mean. It turns out that the variance in the estimate of mean is simply the variance of the sample divided by the number of observations in the sample. It follows then that the standard deviation of estimate of our mean (also known as the *standard error of mean*) is the standard deviation of the sample divided by the square root of the number of observations. Thus, as our sample size increases, the standard error of mean decreases increasing our confidence in the estimate of mean.

> *Standard error of mean is equal to standard deviation of the sample divided by the square root of the number of observations*

Let's calculate *SE of mean* of our two samples:

```{r}
semean_hwy_4 <- signif(sd(pull(hwy_4, hwy)) / sqrt(length(pull(hwy_4, hwy))), 3)
semean_hwy_4
semean_hwy_6 <- signif(sd(pull(hwy_6, hwy)) / sqrt(length(pull(hwy_6, hwy))), 3)
semean_hwy_6
```

This is interesting. From a single sample of data, we know the estimate of population mean and the variance in this estimate. Is it possible to know more about what this population of sample means look like. In other words, if I could take not just one but 10 or 100 or thousands  of samples, calculate mean of each sample and plot a histogram, what would the distribution of those means look like? The answer comes from the mighty *Central Limit Theorem*.

### Central Limit Theorem
Central Limit Theorem (CLT) says that regrdless of what the distribution of individual observations of a certain population looks like, the distribution of sample means gets closer and closer to *normal distribution* as the number of observations increase. 

> *Distribution of averages of a random variable samples gets closer and closer to normal distribution as the sample size increases.*

### Key Facts about Normal Distribution
Now that we know that the distribution of averages follows normal distribution, it will help to know a little bit about this normal distribution. Normal distribution has a *bell shape* and is also referred to as *Gaussian Distribution*. Due to the bell shape, more observations are concentrated at the center (mean) and the density decreases as we move away from the center in either direction.

* A normal distrubution with a mean of 0 and standard deviation of 1 is known as *standard normal distribution*.
* 68%, 95% and 99% of the normal density lies within approximately 1, 2 and 3 standard deviations from the mean.
* -1.96, -1.645, 1.645, 1.96 are the 2.5th, 5th, 95th and 97.5th percentiles of the standard normal distribution.

These facts allow us to build confidence intervals for any estimate that follows a normal distribution.

### Confidence Intervals for the Estimate of Mean
Now we have the knowledge to create confidence intervals for our estimate of mean. For example, since the mean follow normal distribution (per CLT), 95% of the means will lie within approximately *+/- 2 x SE*:

```{r}
CI_4 <- mean(pull(hwy_4)) + c(-2, 2) * semean_hwy_4
signif(CI_4, 3)
CI_6 <- mean(pull(hwy_6)) + c(-2, 2) * semean_hwy_6
signif(CI_6, 3)
```
**What does these confidence intervals mean?**

One way to interpret the confidence interval is that we are 95% confident that the population lies in this interval. But more fundamental way to think is that if we take 100 4-cylinder samples, and construct confidence intervals using the above calculated values, approximately 95 of those intervals will contain the population mean. 

### Calculation of Confidence Intervals using R Functions
We can directly use R functions (`pnorm` and `qnorm`) to calculate quantities for normal distributions. Let's look at some examples:

**The mean SAT score is 1083 with a standard deviation of 194. What is the probability that a randomly drawn student from this population has a score of 800 or less?**

Notice here that we are directly given the population parameters. We don't have a single sample. So assuming this population is normally distributed, we are asked to find the area bounded by the curve representing this distribution, x-axis and the left of the vertical line drawn at 800 x-axis point. This can be computed by:

```{r}
pnorm(800, 1083, 194)
```
So we can say that there is less than 8% chance of randomly picking a student with a SAT score of 800 or less from this population.

**What is the probability that a randomly drawn student from this population has a score of 1150 or more?**

Now we need the area to the *right* of vertical line drawn at 1150. So we will use the argument **lower.tail = FALSE** to answer this question:

```{r}
pnorm(1150, 1083, 194, lower.tail = FALSE)
```

So there is about 36% chance of picking a student with a score of 1150 or more. Now let's ask the opposite question.

**What range of scores cover 95% of the population?**

In order to cover 95% of the population, we have to leave 2.5% area on either side of the curve. This means we have to compute 2 quantiles given two probabilities - .025 and .975

```{r}
qnorm(c(.025, .975), 1083, 194)
```
Now let's apply the same calculation to our 4-cylinder data:

```{r}
signif(qnorm(c(.025, .975), 28.8, semean_hwy_4), 3)
```

This is pretty close to the calculation we did earlier using the fact the 95% of the range is covered by approximately 2 standard deviations around the mean.

### Hypothesis Testing - A Systematic Approach to Compare Sample Data

Now that we know how to find confidence intervals, we need a formal testing procedure to evaluate if a sample is really different from some assumed population or not. Let's pose a simple question:

*Did the sample came from the same population?*

In this situation, a population is known and we want to compare our sample and infer if the sample came from the same population or a different one. Let's assume for the sake of argument here that population mean for all 4 cylinder cars is 27 mpg. The question is how likely is that our sample came from this population. This and similar type if questions can be tested more systematically using *hypothesis testing* framework. The basic idea is we create two hypotheses:

* **Null Hypothesis (Ho)**: The sample came from same population. This is the status quo. 
* **Alternate Hypothesis (Ha)**: The sample came from a different population.
* Then we assume Ho to be true. 
* Then we look at the sample data and try to see if there is *enough evidence to reject Ho*. We do this by calulating probabilty of getting the mean as or more extreme as we did in our sample.
* If the probability is less than a certain threshold value, we say that it is unlikely to get a mean as or more extreme as we did in our sample and thus reject the null hypothesis. 
* If the probability is not less than the threshold we chose, then we say we *fail to reject null*.

In our fuel economy sample, Ho is that population mean from which the sample is drawn is 27 mpg. Ha is that the population mean from which sample is drawn is *not* 27 mpg. Let's find the probability of obtaining a mean as extreme or more than 28.8 mpg given that population mean is 27 mpg. 

```{r}
2 * pnorm(28.8, 27, semean_hwy_4, lower.tail = FALSE)
```

We multiplied by 2 to calculate *two-sided* probability since Ha that we are evaluating states *not equal* to 27 mpg. If Ha was defined to be *more* or *less* than 27 mpg, then we would have used *one-sided* probability. So there is 0.03% chance that we would observe the mean that we did assuming Ho to be true. A typical threshold is 5%, considering which we reject the null and say that the sample is extremely unlikely to have come from the population with a mean of 27 mpg.

**What is a *p-value* ?**

What we just computed above (.03%) is called a p-value. The threshold is called *alpha*. In these terms, we reject Ho if p-value is less than alpha and fail to reject if it is equal to or greater than alpha. Can I ever run into a situation where I incorrectly reject (or fail to reject) Ho? We are never 100% certain of our conclusions so, yes, you can!

### Potholes in Hypothesis Testing

Now what if my p-value is less than 0.05 and I reject Ho but in reality Ho was true? In that case, we made an error which in statistical circles is commonly known as **Type 1 Error**. This can happen because we always choose a non-zero value of alpha, which controls the Type 1 error rate. Alpha = 0.05 means that there will be about 5% cases where I will incorrectly reject Ho.

Since I want to keep Type 1 error as small as possible, I want to keep the value of alpha as small as I can. What is the smallest I can go on alpha? Theoretically, you can go as low as 0 giving you a Type 1 error rate of 0. But this would lead to a problem. Since p-value is the probability and thus cannot go negative, we will never reject Ho in this case, no matter how far my observed mean is from Ho! Now we will run into another problem when Ha is true and we are committing another error known as **Type 2 Error** - the probability of failing to reject Ho when in fact Ho is false.

Ideally, we want to minimize both Type 1 and 2 errors. Type 2 error is called *Beta*. But we ususally don't think in terms of Beta. Instead we think in terms of *Power*. 

### What is Power?

*Power* is the probability of rejecting Ho when it is false. This is *1 - Beta*. To calculate power we need to know what the distribution under Ha looks like. Now this is sort of 'hypothetical problem' (this is because Ha is not defined precisely, but rather in terms of inequality). Recall that Ha is stated as *less than/not equal to/greater than* so we don't know what the mean is under Ha. 

```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("../images/power_hypothesis.png")
```

Power instead is used to design experiments. We first ask ourselves, what effect size do we want to measure? For example, average price of books from one seller is \$50.00 and we want to compare if another seller has a price difference of \$0.50, \$1.00 or \$5.00. This results in an interplay between:

* **Effect size**: The larger the size, the more power you have (easier to identify the effect)
* **Sample size**: Recall that SE of mean is inversally proportional to square root of sample size. So larger the sample size, more squeezed will be the distribution of means towards the center. This will result in less overlap between Ho and Ha distribution (for the same effect size) and thus more power.
* **Alpha**: Since power is the probability of rejecting Ho when it is false, it is the area under the Ha curve beyond the alpha value. So as we decrease alpha (in an effort to reduce type 1 error rate), we also decrease the area under Ha, thus decreasing the power.

We will perform these calculations assuming *t-distribution* which is discussed next.

### Small Sample?
So far we have been assuming that our sample size is reasonably large (typically greater than 30) for CLT to be valid. We may come across problems where sample size is not sufficiently large. In those cases, we simply switch from assuming a normal distribution for our data to assuming a **Student t Distribution**. *t-distribution* takes only one parameter, *degrees of freedom* (n-1) and we replace our *pnorm* and *qnorm* functions with `pt` and `qt` for calculating the probabilities and quantiles.

```{r}
CI_4_t <- mean(pull(hwy_4)) + qt(c(.025, .975), 80) * semean_hwy_4
CI_4_t
```
Note that there is no direct function to input mean and se inside *qt* function so we first calculate the vector of relevant quantiles from *df*, then multiply it by SE of estimate and then add this to the mean of the sample. Also note that in this particular case, sample size is fairly large, and thus the confidence interval predicted by t-distribution is pretty close to those computed from the normal distribution assumption.

Since t-distribution works well for both small and large samples, a general rule of thumb is to assume t-distribution instead of normal. Let's try to layout the steps and methods to solve different types of problems using the t-distrubution.

### Problems

Here we identify typical questions/problems, go through the steps needed to solve the problem and finally use `t.test` function for a more compact form of our solution method.

**Given a vector of random numbers (also called a *single sample*), what is the 95% confidence interval for mean?**

*Using systematic calculation steps*

* Step 1: Compute sample mean
* Step 2: Compute sample standard deviation
* Step 3: Compute standard error of mean
* Step 4: Compute degrees of freedom
* Step 5: Compute t-statistic
* Step 6: Compute confidence interval
* Step 7: State your finding

Let's apply this method to our 4-cylinder data:

```{r}
hwy_4 <- pull(hwy_4)    # convert tibble to vector
s_mean <- mean(hwy_4)
s_sd <- sd(hwy_4)
se_mean_4 <- s_sd / sqrt(length(hwy_4))
df_4 <- length(hwy_4) - 1
ts_4 <- qt(0.025, df_4)
ci_4 <- s_mean + c(1, -1) * ts_4 * se_mean_4
ci_4
```
*Using `t.test`*

Since the output of a single sample `t.test` includes confidence interval, we can directly use that:

```{r}
t.test(hwy_4)
```

And if we just want confidence interval, we can exract that with **conf.int**:

```{r}
t.test(hwy_4)$conf.int
```
*Conclusion: We are 95% confident that our sample mean lies between 27.8 and 29.8 mpg.*

**Given a random vector, what is the chance that it came from a population with mean muo?**

*Using systematic calculation steps*

* Step 1: Define hypothesis:
  Ho: mean = muo
  Ha: mean != muo
* Step 2: Compute sample mean, sample standard deviation, standard error of mean and degrees of freedom
* Step 3: Find T-Score = (sample mean - muo) / SEmean
* Step 4: Find p-value
    (a) if T +ve, 2 * pt(T-score, df, lower.tail = FALSE)
    (b) if T -ve, 2 * pt(T-score, df)
* Step 5: Decide
    (a) if p-value < alpha, reject Ho in favor of Ha
    (b) if p-value > alpha, fail to reject Ho

Let's apply this method to our 4-cylinder data using muo as 27 mpg:

```{r}
T_score_4 <- (s_mean - 27) / se_mean_4
T_score_4
p_val_4 <- 2 * pt(T_score_4, df_4, lower.tail = FALSE)
p_val_4
```

*Using `t.test`*
```{r}
t.test(hwy_4 - 27)
```
*Conclusion: Since p-value is less than 0.05 (alpha), we reject Ho in favor of Ha concluding that there is significant difference between our sample and Ho mean of 27 mpg. Also note that in this case also results of our systematic steps match with p-value from t.test validating our steps.*

**Given two samples, where the data from the two samples is *independant*, is there a statistical difference between the two?**

In this case the calculation of the confidence interval gets messy, even if variance between the two samples is assumed equal. It is even more complicated if you don't assume that the variance is equal. That means you will solve a problem once and then easily forget it. Instead, `t.test` can rescue us again:

```{r}
hwy_6 <- pull(hwy_6)    # convert tibble to vector
t.test(hwy_4, hwy_6)
```
By default, *t.test* assumes unequal variance. You can set `var.equal = TRUE` if needed.

*Conclusion: Since p-value is extremely small, we reject the null hypothesis and say that the two samples are not equal. In addition, we are 95% confident that the difference between means of the 4-cylinder vehicles and 6-cylinder vehicles is between 4.7 and 7.3 mpg.*

**Given two samples where the data in the two samples is *paired*, is there a statistical difference between tht two?**

For this problem, let's use `sleep` data available in R from Gosset's Biometrika paper. It shows number of hours of sleep for 10 patients on two separate drugs.

```{r}
data(sleep)
str(sleep)
group1 <- sleep %>%
    filter(group == 1) %>%
    select(extra) %>%
    pull()
group2 <- sleep %>%
    filter(group == 2) %>%
    select(extra) %>%
    pull()
t.test(group2, group1, paired = TRUE)
```

*Conclusion: Since the 95% confidence interval does not include 0, we can say that there is a significant difference between the two drugs.*

**The average fuel economy of a 4-cylinder car is 28 mpg. It is claimed that with the introduction of a new technology, the fuel economy increased to 29 mpg. How many observations should we take to study this claim to get a power of 80% from our test? Assume that the standard deviation in mean mpg is 0.5 mpg and we need 95% confidence interval.**

This can be answered using `power.t.test` function which will compute either power, sample size, change that we are interested in measuring or the standard deviation given the rest. Let's use this to compute sample size in our case:

```{r}
power.t.test(power = 0.8, delta = 1, sd = 0.5, alt = "one.sided", type = "one.sample")
```
*Conclusion: I need 4 samples to distinguish this difference.*

**What is the maximum difference I can detect at 80% power level with sample size of 50 and standard deviation of 1?**

```{r}
power.t.test(power = 0.8, sd = 1, n = 50)
```

*Conclusion: I can detect a maximum delta of 0.56.*

### What if I Want to Study a Test Statistic Other than Mean?

The confidence intervals that we have generated for our hypothesis testing has revolved around means where CLT has helped us a lot. But the CLT does not say anything about a different statistic, for example, median or variance. Sometimes I may be interested in looking at the differences in medians instead. How can I create a confidence interval of median and other statistics? Here, fundamentally we are saying what if my statistic does not follow a normal or a t-distribution. In this regard, it is also applicable to mean if there is a skew in its distribution (becaues assumptions of t-distribution are also violated in the presence of skew).

Here, the *bootstrap technique* (also known as *resampling technique*) will come to our rescue.

**How does it work?**

So we have one sample staring at us. What do we do? Imagine we put all those numbers in a sack. Then we randomly pick one number out, note it down, and *put it back* in the sack. We shuffle the contents of our sack and pick another number, note it down and put it back in the sack again. In statistical circles, this is called *sampling with replacement*. Note that in this experiment, we can get the same number again during our second pick. Now we can have virtually endless supply of numbers, which are coming from the sample, and thus represents the distribution of our sample. This way we can 'create' thousands of samples giving us thousands of any test statistic that we want. We then just need to look at the distribution of our test statistic to create confidence intervals.

**How do we do this in R?**

We will use `sample` function with `replace = TRUE` to generate the number of samples we want. We will then arrange the resulting vector into a  matrix and use the `apply` function to compute our test statistic for all samples. Finally we employ `quantile` function to pick any quantile from our test statistic distribution. Let's use this to calculate 95% CI for mean and then median for our 4-cylinder mpg data:

```{r, out.width = "80%"}
n <- length(hwy_4)
N <- 10000
resamps_hwy_4 <- sample(hwy_4, n * N, replace = TRUE)
resamps_hwy_4_mat <- matrix(resamps_hwy_4, N)
resamp_mean <- apply(resamps_hwy_4_mat, 1, mean)
resamp_median <- apply(resamps_hwy_4_mat, 1, median)
quantile(resamp_mean, c(0.025, 0.975))
quantile(resamp_median, c(0.025, 0.975))
```

Note that our 95% CI for mean agrees very well with our earlier calculations using CLT! But the best part is this is a general technique that can be applied to any statistic that we are interested in. Finally it is a good idea to look at the histogram of both:

```{r, out.width = "80%"}
resamp_mean %>%
    as.tibble() %>%
    ggplot(aes(value)) +
    histogram() +
    theme_simple()

resamp_median %>%
    as.tibble() %>%
    ggplot(aes(value)) +
    histogram() +
    theme_simple()
```

### Permutation Testing - A Different Way of Hypothesis Testing

Permutation testing is a very interesting idea to test if two groups are different. 

**How does it work?**

You start with the hypothesis that the two groups are same (come from the same population). In other words, this means that the 'labels' identifying the two groups are irrelevent. So we compute our test statistic from the original groups. Then we shuffle the labels to create a 'second sample' and calculate our test statistic again. This way we can continue to shuffle and recompute test statistic as many times as we want. This allows us to create a distribution. We can then use this distribution to find what proportion of our test statistic is as or more extreme than our original group. If this is less than a certain threshold, we reject the null hypotesis and say that the two groups are different.

**How do we do this in R?**

Let's apply permutation testing to find if 4-cyl data is different from 6-cyl data. We first generate the label vector (cyl). Then a function is created that does the subsetting based on the label and calcuated difference in means of the two groups. Labels are then permuted using `sample` and custom function is applied 10,000 times. Finally the fraction of the permutation that are greater than the acutal difference is calculated.

```{r, out.width = "80%"}
hwy_mpg <- c(hwy_4, hwy_6)
cyl <- c(rep("4-cyl", length(hwy_4)), rep("6-cyl", length(hwy_6)))
delta <- function(x, y) mean(x[y == "4-cyl"]) - mean(x[y == "6-cyl"])
actual_diff <- delta(hwy_mpg, cyl)
actual_diff
permute_diff <- sapply(1: 10000, function(i) delta(hwy_mpg, sample(cyl)))
mean(permute_diff > actual_diff)
permute_diff %>%
    as.tibble() %>%
    ggplot(aes(value)) +
    histogram() +
    theme_simple()
```

It is clear that the permuted difference has a mean of around 0 whereas the actual difference is close to 6 mpg. Since the proportion of permuted differences that are as extreme or more than observed value is 0, we can reject the null hypothesis that the two groups are identical in favor of the alternative that the groups are different.

### Summary

We have covered a good amount of ground by going through fundamental concepts of inference. We have consciously kept the focus only on numerical vectors here to avoid getting distracted by although similar ideas but different distributions for categorical outputs. But extending the concepts we have learned here to categorical variables should be fairly straightforward.



