---
title: "Statistical Inference Using R"
author: "Paramjot Singh"
date: 2018-03-22T03:40:34-05:00
draft: FALSE
tags: ["R"]
categories: ["Fundamentals"]
output:
  blogdown::html_page:
    toc: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  comment = "#>",
  class.source = 'white',
  fig.align = 'center'
)
```
Imagine you were in the market for a new car. You care about running cost of the car and thus one of the buying criteria you have in mind is better fuel economy. One of your friend suggests to buy a 4-cylinder car instead of 6 or 8 cylinders since larger engine size ususally translates to worse fuel economy. You are a data scientist so you just don't take his word but want to look at some concrete data before you narrow down your search to 4-cylinder vehicles. You find this `mpg` dataset in one of the R packages, `ggplot2` and start to look at it to answer your question:

> Do 4-cylinder cars have better fuel economy than 6-cylinder cars?

This is a type of question that statistical inference will help us answer. In this post, we will learn the fundamental concepts related to inference. As usual, we will keep the focus tight on the minimum set of tools needed to start applying the concepts to data at hand. We will add more tools as we start to play with different types of data.

### Everything Varies
Variation is inherent to all phenomenon and processes. Otherwise our job is done. Our objective is to understand the variation in data so we can make an informed decision. So we want to 'quantify' the variation in our data. Just looking at the raw data is not going to cut it. Let's try:

```{r}
library(tidyverse)
hwy_4 <- mpg %>% 
    filter(cyl == 4) %>% 
    select(hwy)
hwy_6 <- mpg %>% 
    filter(cyl == 6) %>% 
    select(hwy)
hwy_4
hwy_6
```

It is not a bad idea to look at the raw data but it is difficult to conclude anything. Surprisingly, the fuel economy numbers are rounded to 2 significant figures (and thus of type *integer*). Generally, mpg numbers are provided with 3 decimal places. It appears that 4-cylinder vehicles have slightly better fuel economy but there is some overlap too. Moreover, our brain will start to complain if we stare at 80 data points for each set for too long. And sometimes we may have to work with many more observations. One tool to ease out that pain is plots. Let's look at the histogram of each set:

```{r}
library(themeSimple)
ggplot(hwy_4, aes(hwy)) +
    histogram(binwidth = 3) + 
    xlim(15, 45) +
    theme_simple()

ggplot(hwy_6, aes(hwy)) +
    histogram(binwidth = 3) +
    xlim(15, 45) +
    theme_simple()
```
I kept the x-axis scale same in both plots so we can compare the two easily. It is apparent that the mean of 4-cylinder vehicles is centered aroung 30 mpg (miles per gallon) while it is around 25 mpg for 6-cylinder vehicles. So should you conclude that 4-cylinder cars are better? Well, let's wait until we get some quantifiable differences.

### Mean - One Measure of Central Tendency
To quantify the difference, one measure we can consider is *mean*, which is a measure of central tendency of the data:

```{r}
mpg %>% 
    group_by(cyl) %>%
    filter(cyl == 4 | cyl == 6) %>%
    summarize(mean_hwy = mean(hwy))
```
So the mean fuel economy of 4-cylinder cars is greater than 6-cylinder cars by 6 mpg! That's quite a bit. And now we have quantified the difference. Can we conclude now that 4-cylinder cars are better than 6-cylinders? Well not so fast!

### Sample vs Population
One thing is clear that your friend is not going to pick up one of the cars used to generate this data. For one, the latest model (2008) in the data set is 10 years old. The data we have with us is called *sample* and the mean we have calculated is called *sample mean*. This sample comes from *population* that in this case consists of all the 4 and 6 cylinder cars manufactured in US from year 1999 to 2008. What we really want to know is *population mean*. But there is no way to exactly know the population mean (unless you are ready to measure mpg for all the cars, which is a costly proposition!). The only option we have is to *estimate* population mean from sample mean. 

### Estimating a Population Parameter from a Sample Quantity
**How do we estimate population mean from sample mean?**

The answer is quite simple here.

> Sample mean is an *unbiased estimator* of population mean.

Sample mean is an *unbiased* estimator of population mean so we don't have to worry too much about the estimate here. What we need to answer is *how confident* are we in our estimate? It is likely that if we collect another sample, the mean of that sample is not going to be exactly same as the first. This is because since the individual observations are *random numbers*, anything that we calculate from those will also be a random number. To understand the variability in mean, we need to know a bit about *variance*.

### Variance - A Measure of Spread
While mean gives you central tendency of a population, *variance* represents the spread or distribution of observations around the mean. We typically think of variance in terms of *standard deviation* which is just the square root of variance. This is because the units of standard deviation matches with the data we are analyzing. So in short, a data with standard deviation of 5 will be more spread out compared to a data with standard deviation of 1. Let's look at the histogram of these two sets:

```{r}
low_var <- rnorm(1000)
high_var <- rnorm(1000, mean = 0, sd = 5)
data <- tibble(id = rep(c("low", "high"), each = 1000), 
               obs = c(low_var, high_var))
ggplot(data, aes(x = obs)) +
    histogram(data = filter(data, id == "high"), 
              fill = 'darkorange', 
              alpha = 0.8) +
    histogram(data = filter(data, id == "low"), 
              alpha = 0.8) +
    theme_simple()
```

### What is the Variance in Estimate of Mean
Now that we know what variance is, we need to find what is the variance in our estimate of mean. It turns out that the variance in the estimate of mean is simply the variance of the sample divided by the number of observations in the sample. It follows then that the standard deviation of estimate of our mean (also known as the *standard error of mean*) is the standard deviation of the sample divided by the square root of the number of observations. Thus, as our sample size increases, the standard error of mean decreases increasing our confidence in the estimate of mean.

> *Standard error of mean is equal to standard deviation of the sample divided by the square root of the number of observations*

Let's calculate *SE of mean* of our two samples:

```{r}
semean_hwy_4 <- signif(sd(pull(hwy_4, hwy)) / sqrt(length(pull(hwy_4, hwy))), 3)
semean_hwy_4
semean_hwy_6 <- signif(sd(pull(hwy_6, hwy)) / sqrt(length(pull(hwy_6, hwy))), 3)
semean_hwy_6
```

This is interesting. From a single sample of data, we know the estimate of population mean and the variance in this estimate. Is it possible to know more about what this population of sample means look like. In other words, if I could not take just one sample, but 10, 100 or thousands  of samples, calculate mean of each sample and plot a histogram, what would the distribution of those means look like? The answer is the mighty *Central Limit Theorem*.

### Central Limit Theorem
Central Limit Theorem (CLT) says that regrdless of what the distribution of individual observations of a certain population looks like, the distribution of sample means gets closer and closer to *normal distribution* as the number of observations increase. 

> *Distribution of averages of a random variable samples gets closer and closer to normal distribution as the sample size increases.*

### Key Facts about Normal Distribution
Now that we know that the distribution of averages follows normal distribution, it will help to know a little bit about this normal distribution. Normal distribution has a *bell shape* and is also referred to as *Gaussian Distribution*. Due to the bell shape, more observations are concentrated at the center (mean) and the density decreases as we move away from the center in either direction.

* A normal distrubution with a mean of 0 and standard deviation of 1 is known as *standard normal distribution*.
* 68%, 95% and 99% of the normal density lies within approximately 1, 2 and 3 standard deviations from the mean.
* -1.96, -1.645, 1.645, 1.96 are the 2.5th, 5th, 95th and 97.5th percentiles of the standard normal distribution.

These facts allow us to build confidence intervals for any estimate that follows a normal distribution.

### Confidence Intervals for the Estimate of Mean
Now we have the knowledge to create confidence intervals for our estimate of mean. For example, since the mean follow normal distribution (per CLT), 95% of the means will lie within *+/- 2 x SE*:

```{r}
CI_4 <- mean(pull(hwy_4)) + c(-2, 2) * semean_hwy_4
signif(CI_4, 3)
CI_6 <- mean(pull(hwy_6)) + c(-2, 2) * semean_hwy_6
signif(CI_6, 3)
```
**What does these confidence intervals mean?**

One interpretation of confidence interval is that there is 95% chance that the population mean will lie within 27.8 and 29.8 mpg for 4-cylinder cars. In other words, these confidence intervals mean that if we take 100 4-cylinder samples, 95 of those means will contain the population mean. 
